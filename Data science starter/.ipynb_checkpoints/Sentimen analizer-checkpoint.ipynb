{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ab12592-448c-4eb6-af1f-cf813f05c47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_docs = [\" Here are some very simple basic sentences .\" ,\n",
    "\" They won ’t be very interesting , I ’m afraid .\" ,\n",
    "\" The point of these examples is to _learn how basic text \\\n",
    "cleaning works_ on * very simple * data .\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5eee8be3-3352-4380-8a0e-074e93cc52d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mamto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import nltk\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d7e36b68-4065-4c9c-9ad0-9a3cec6a71f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Here', 'are', 'some', 'very', 'simple', 'basic', 'sentences', '.'], ['They', 'won', '’', 't', 'be', 'very', 'interesting', ',', 'I', '’', 'm', 'afraid', '.'], ['The', 'point', 'of', 'these', 'examples', 'is', 'to', '_learn', 'how', 'basic', 'text', 'cleaning', 'works_', 'on', '*', 'very', 'simple', '*', 'data', '.']]\n"
     ]
    }
   ],
   "source": [
    "from nltk . tokenize import word_tokenize\n",
    "tokenized_docs = [ word_tokenize ( doc ) for doc in raw_docs ]\n",
    "print (tokenized_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef878c24-bfd0-43ce-b67f-b608d9abe1d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string . punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f164d8f6-fda3-4e2d-8901-8cffafe220a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Here', 'are', 'some', 'very', 'simple', 'basic', 'sentences'], ['They', 'won', '’', 't', 'be', 'very', 'interesting', 'I', '’', 'm', 'afraid'], ['The', 'point', 'of', 'these', 'examples', 'is', 'to', 'learn', 'how', 'basic', 'text', 'cleaning', 'works', 'on', 'very', 'simple', 'data']]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "regex = re . compile ('[%s]'  % re . escape ( string .punctuation ))\n",
    "tokenized_docs_no_punctuation = []\n",
    "for review in tokenized_docs :\n",
    "    new_review = []\n",
    "    for token in review :\n",
    "        new_token = regex . sub (u'', token )\n",
    "        if not new_token == u'' :\n",
    "            new_review . append ( new_token )\n",
    "    tokenized_docs_no_punctuation . append ( new_review)\n",
    "print (tokenized_docs_no_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8ba51fed-cbb1-41a7-82b0-ad6c39195a2c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15052/1032830141.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# wordnet = WordNetLemmatizer ()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# each of the following commands perform stemming on word\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mporter\u001b[0m \u001b[1;33m.\u001b[0m \u001b[0mstem\u001b[0m \u001b[1;33m(\u001b[0m \u001b[0mword\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;31m# snowball . stem ( word )\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# wordnet . lemmatize ( word )\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\HaiEnv\\lib\\site-packages\\nltk\\stem\\porter.py\u001b[0m in \u001b[0;36mstem\u001b[1;34m(self, word, to_lowercase)\u001b[0m\n\u001b[0;32m    653\u001b[0m         \u001b[1;33m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mto_lowercase\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mif\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mto_lowercase\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mword\u001b[0m \u001b[0malways\u001b[0m \u001b[0mlowercase\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    654\u001b[0m         \"\"\"\n\u001b[1;32m--> 655\u001b[1;33m         \u001b[0mstem\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mto_lowercase\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNLTK_EXTENSIONS\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "from nltk . stem . porter import PorterStemmer\n",
    "from nltk . stem . snowball import SnowballStemmer\n",
    "from nltk . stem . wordnet import WordNetLemmatizer\n",
    "word = tokenized_docs_no_punctuation\n",
    "porter = PorterStemmer ()\n",
    "# snowball = SnowballStemmer (’ english ’)\n",
    "# wordnet = WordNetLemmatizer ()\n",
    "# each of the following commands perform stemming on word\n",
    "porter . stem ( word )\n",
    "# snowball . stem ( word )\n",
    "# wordnet . lemmatize ( word )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8adb5213-d89d-4bed-b2a7-7658a1eab80b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Original text : \n",
      " <p > While many of the stories tuggedat the heartstrings , I never felt manipulated bythe authors . ( Note : Part of the reason why I don't like the 'Chicken Soup for the Soul series is that I feel that the authors are justdying to make the reader clutch for the box oftissues .) </p >\n",
      " Cleaned text : \n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "To remove HTML markup, use BeautifulSoup's get_text() function",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15052/3628094983.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtest_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\" Cleaned text : \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mnltk\u001b[0m \u001b[1;33m.\u001b[0m \u001b[0mclean_html\u001b[0m \u001b[1;33m(\u001b[0m \u001b[0mtest_string\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Anaconda3\\envs\\HaiEnv\\lib\\site-packages\\nltk\\util.py\u001b[0m in \u001b[0;36mclean_html\u001b[1;34m(html)\u001b[0m\n\u001b[0;32m    736\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    737\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mclean_html\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 738\u001b[1;33m     raise NotImplementedError(\n\u001b[0m\u001b[0;32m    739\u001b[0m         \u001b[1;34m\"To remove HTML markup, use BeautifulSoup's get_text() function\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    740\u001b[0m     )\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: To remove HTML markup, use BeautifulSoup's get_text() function"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "test_string =\" <p > While many of the stories tugged\\\n",
    "at the heartstrings , I never felt manipulated by\\\n",
    "the authors . ( Note : Part of the reason why I don't like the 'Chicken Soup for the Soul \\\n",
    "series is that I feel that the authors are just\\\n",
    "dying to make the reader clutch for the box of\\\n",
    "tissues .) </a >\"\n",
    "print(\" Original text : \")\n",
    "print (test_string)\n",
    "print (\" Cleaned text : \")\n",
    "nltk . clean_html ( test_string )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "67998c03-3850-4b78-aeb8-ee950e4d7d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('Mireia', 1), ('loves', 1), ('me', 2), ('more', 1), ('than', 1), ('Hectorloves', 1)])\n",
      "dict_items([('Sergio', 1), ('likes', 1), ('me', 2), ('more', 1), ('than', 1), ('Mireia', 1), ('loves', 1)])\n",
      "dict_items([('He', 1), ('likes', 1), ('basketball', 1), ('more', 1), ('than', 1), ('football', 1)])\n"
     ]
    }
   ],
   "source": [
    "mydoclist = [  \"Mireia loves me more than Hectorloves me\", \"Sergio likes me more than Mireia loves me\",\"He likes basketball more than football\" ]\n",
    "from collections import Counter\n",
    "for doc in mydoclist :\n",
    "    tf = Counter ()\n",
    "    for word in doc . split () :\n",
    "        tf [ word ] += 1\n",
    "    print (tf . items ())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6cabf070-4a5d-4ec0-ab0c-9d86a38867d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Counter () # a new , empty counter\n",
    "c = Counter (  \"gallahad\" ) # a new counter from an iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e3977a90-5200-42c5-808d-cedd4d0d2cf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = Counter ([ \"eggs\", \"ham\"])\n",
    "c[  \"bacon\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bd035d30-23eb-403f-8cda-02bfcdcbcd4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our vocabulary vector is [Hectorloves, He, more, basketball, Mireia, me, Sergio, loves, football, than, likes]\n",
      "The doc is \"Mireia loves me more than Hectorloves me'\n",
      "The tf vector for Document 1 is [1, 0, 1, 0, 1, 2, 0, 1, 0, 1, 0] \n",
      "The doc is \"Sergio likes me more than Mireia loves me'\n",
      "The tf vector for Document 2 is [0, 0, 1, 0, 1, 2, 1, 1, 0, 1, 1] \n",
      "The doc is \"He likes basketball more than football'\n",
      "The tf vector for Document 3 is [0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1] \n",
      "All combined , here is our master document term matrix : \n",
      "[[1, 0, 1, 0, 1, 2, 0, 1, 0, 1, 0], [0, 0, 1, 0, 1, 2, 1, 1, 0, 1, 1], [0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "def build_lexicon ( corpus ):\n",
    "# define a set with all possible words included in all the sentences or \" corpus \"\n",
    "    lexicon = set ()\n",
    "    for doc in corpus :\n",
    "        lexicon . update ([ word for word in doc . split() ])\n",
    "    return lexicon\n",
    "def tf ( term , document ):\n",
    "    return freq ( term , document )\n",
    "\n",
    "def freq ( term , document ):\n",
    "    return document . split () . count ( term )\n",
    "\n",
    "vocabulary = build_lexicon ( mydoclist )\n",
    "doc_term_matrix = []\n",
    "print (\"Our vocabulary vector is [\"  + \", \". join ( list ( vocabulary )) + \"]\" )\n",
    "\n",
    "for doc in mydoclist :\n",
    "    print ('The doc is \"'  + doc + \"'\")\n",
    "    tf_vector = [ tf ( word , doc ) for word in vocabulary ]\n",
    "    tf_vector_string = \", \". join ( format ( freq , \"d\") for freq in tf_vector )\n",
    "    print (\"The tf vector for Document %d is [% s] \" % (( mydoclist . index ( doc ) +1) , tf_vector_string ))\n",
    "    doc_term_matrix . append ( tf_vector )\n",
    "print (\"All combined , here is our master document term matrix : \")\n",
    "print( doc_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4b029e-f3f9-4fda-adfb-cb775f6afc80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
